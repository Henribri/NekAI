{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d8dd83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:26:41.263746Z",
     "iopub.status.busy": "2022-10-30T16:26:41.263060Z",
     "iopub.status.idle": "2022-10-30T16:26:47.522207Z",
     "shell.execute_reply": "2022-10-30T16:26:47.521293Z"
    },
    "papermill": {
     "duration": 6.268412,
     "end_time": "2022-10-30T16:26:47.524736",
     "exception": false,
     "start_time": "2022-10-30T16:26:41.256324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import math\n",
    "import codecs\n",
    "import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a96563",
   "metadata": {
    "papermill": {
     "duration": 0.003921,
     "end_time": "2022-10-30T16:26:47.533123",
     "exception": false,
     "start_time": "2022-10-30T16:26:47.529202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preparation üë®‚Äçüç≥\n",
    "## Get lyrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01273d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics =codecs.open('all_lyrics.txt', 'r', 'utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d689d",
   "metadata": {},
   "source": [
    "## Reduce vocabulary by replacing character\n",
    "Only keep lowcase letters and space to make learning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a117e81e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:26:47.543159Z",
     "iopub.status.busy": "2022-10-30T16:26:47.542410Z",
     "iopub.status.idle": "2022-10-30T16:26:48.095430Z",
     "shell.execute_reply": "2022-10-30T16:26:48.094467Z"
    },
    "papermill": {
     "duration": 0.560652,
     "end_time": "2022-10-30T16:26:48.097773",
     "exception": false,
     "start_time": "2022-10-30T16:26:47.537121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "simplified_lyrics=unidecode.unidecode(lyrics.lower())\n",
    "simplified_lyrics = simplified_lyrics.replace('!','')\n",
    "simplified_lyrics = simplified_lyrics.replace('\"','')\n",
    "simplified_lyrics = simplified_lyrics.replace(\"'\",'')\n",
    "simplified_lyrics = simplified_lyrics.replace('+','')\n",
    "simplified_lyrics = simplified_lyrics.replace('-','')\n",
    "simplified_lyrics = simplified_lyrics.replace('<','')\n",
    "simplified_lyrics = simplified_lyrics.replace('>','')\n",
    "simplified_lyrics = simplified_lyrics.replace('.','')\n",
    "simplified_lyrics = re.sub(r'\\d', '#', simplified_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2218e54",
   "metadata": {
    "papermill": {
     "duration": 0.003951,
     "end_time": "2022-10-30T16:26:48.106369",
     "exception": false,
     "start_time": "2022-10-30T16:26:48.102418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a70bfd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:26:48.117198Z",
     "iopub.status.busy": "2022-10-30T16:26:48.115733Z",
     "iopub.status.idle": "2022-10-30T16:26:48.137861Z",
     "shell.execute_reply": "2022-10-30T16:26:48.136978Z"
    },
    "papermill": {
     "duration": 0.029381,
     "end_time": "2022-10-30T16:26:48.139949",
     "exception": false,
     "start_time": "2022-10-30T16:26:48.110568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '#', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab=sorted(set(simplified_lyrics))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4554234",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Dictionnaries of conversion\n",
    "Dictionnaries to interchange integers and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70922eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:26:48.158513Z",
     "iopub.status.busy": "2022-10-30T16:26:48.157961Z",
     "iopub.status.idle": "2022-10-30T16:26:48.175149Z",
     "shell.execute_reply": "2022-10-30T16:26:48.174191Z"
    },
    "papermill": {
     "duration": 0.024238,
     "end_time": "2022-10-30T16:26:48.177002",
     "exception": false,
     "start_time": "2022-10-30T16:26:48.152764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ca03a",
   "metadata": {},
   "source": [
    "### Tokenize text\n",
    "Transform characters to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d63514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:26:48.187332Z",
     "iopub.status.busy": "2022-10-30T16:26:48.186709Z",
     "iopub.status.idle": "2022-10-30T16:26:48.264453Z",
     "shell.execute_reply": "2022-10-30T16:26:48.263647Z"
    },
    "papermill": {
     "duration": 0.085156,
     "end_time": "2022-10-30T16:26:48.266591",
     "exception": false,
     "start_time": "2022-10-30T16:26:48.181435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maintenant  ->  [14, 2, 10, 15, 21, 6, 15, 2, 15, 21]\n"
     ]
    }
   ],
   "source": [
    "num_lyrics=[char_to_int[c] for c in simplified_lyrics]\n",
    "\n",
    "print(simplified_lyrics[:10], ' -> ', num_lyrics[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e205f",
   "metadata": {},
   "source": [
    "## Make sequences with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4936f22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:26:48.292839Z",
     "iopub.status.busy": "2022-10-30T16:26:48.292584Z",
     "iopub.status.idle": "2022-10-30T16:26:59.179871Z",
     "shell.execute_reply": "2022-10-30T16:26:59.178874Z"
    },
    "papermill": {
     "duration": 10.895034,
     "end_time": "2022-10-30T16:26:59.182419",
     "exception": false,
     "start_time": "2022-10-30T16:26:48.287385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 80\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(num_lyrics) - seq_length, 1):\n",
    "    seq_in = simplified_lyrics[i:i + seq_length]\n",
    "    seq_out = simplified_lyrics[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e983793",
   "metadata": {
    "papermill": {
     "duration": 0.004324,
     "end_time": "2022-10-30T16:26:59.191538",
     "exception": false,
     "start_time": "2022-10-30T16:26:59.187214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create dataset üìÄ\n",
    "Large bacth size to be able to learn on all dataset due to lack of ressources, moreover it increases the perfomrence for text generation.\n",
    "\n",
    "Don't shuffle dataset to keep consistency of the data.\n",
    "\n",
    "<b> Information : http://ceur-ws.org/Vol-3027/paper108.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ff6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dataX, dataY))\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc81704",
   "metadata": {
    "papermill": {
     "duration": 0.004657,
     "end_time": "2022-10-30T16:30:49.395273",
     "exception": false,
     "start_time": "2022-10-30T16:30:49.390616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define model üí°\n",
    "Use of Bidirectional LSTM layers to learn from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb23de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:30:49.405118Z",
     "iopub.status.busy": "2022-10-30T16:30:49.404829Z",
     "iopub.status.idle": "2022-10-30T16:30:50.297083Z",
     "shell.execute_reply": "2022-10-30T16:30:50.296110Z"
    },
    "papermill": {
     "duration": 0.899706,
     "end_time": "2022-10-30T16:30:50.299329",
     "exception": false,
     "start_time": "2022-10-30T16:30:49.399623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 256)           7168      \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 80, 256)          394240    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                7196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 802,844\n",
      "Trainable params: 802,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "vector_size=256\n",
    "\n",
    "model=keras.Sequential([\n",
    "    layers.Embedding(vocab_size, vector_size, input_length=seq_length),\n",
    "    layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n",
    "    layers.Bidirectional(keras.layers.LSTM(128, return_sequences=False)),\n",
    "    layers.Dense(vocab_size, activation='softmax')      \n",
    "])\n",
    "\n",
    "model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e52adc",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.004921,
     "end_time": "2022-10-30T16:30:50.309027",
     "exception": false,
     "start_time": "2022-10-30T16:30:50.304106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train model ‚è≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7173751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T16:30:50.319196Z",
     "iopub.status.busy": "2022-10-30T16:30:50.318893Z",
     "iopub.status.idle": "2022-10-30T23:21:14.235931Z",
     "shell.execute_reply": "2022-10-30T23:21:14.235022Z"
    },
    "papermill": {
     "duration": 24623.924576,
     "end_time": "2022-10-30T23:21:14.238134",
     "exception": false,
     "start_time": "2022-10-30T16:30:50.313558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 16:30:55.245858: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-30 16:30:56.633638: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 110s 193ms/step - loss: 2.2231\n",
      "Epoch 2/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.8277\n",
      "Epoch 3/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.6977\n",
      "Epoch 4/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.6210\n",
      "Epoch 5/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.5643\n",
      "Epoch 6/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.5213\n",
      "Epoch 7/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.4871\n",
      "Epoch 8/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.4585\n",
      "Epoch 9/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.4337\n",
      "Epoch 10/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.4119\n",
      "Epoch 11/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.3925\n",
      "Epoch 12/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.3746\n",
      "Epoch 13/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.3581\n",
      "Epoch 14/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.3427\n",
      "Epoch 15/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 1.3280\n",
      "Epoch 16/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 1.3135\n",
      "Epoch 17/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.2996\n",
      "Epoch 18/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.2864\n",
      "Epoch 19/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 1.2730\n",
      "Epoch 20/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.2605\n",
      "Epoch 21/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.2541\n",
      "Epoch 22/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.2387\n",
      "Epoch 23/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 1.2264\n",
      "Epoch 24/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.2153\n",
      "Epoch 25/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.2051\n",
      "Epoch 26/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1959\n",
      "Epoch 27/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1875\n",
      "Epoch 28/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.1786\n",
      "Epoch 29/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 1.1688\n",
      "Epoch 30/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1605\n",
      "Epoch 31/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 1.1542\n",
      "Epoch 32/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.1472\n",
      "Epoch 33/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.1394\n",
      "Epoch 34/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 1.1332\n",
      "Epoch 35/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.1273\n",
      "Epoch 36/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1220\n",
      "Epoch 37/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 1.1141\n",
      "Epoch 38/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1121\n",
      "Epoch 39/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1122\n",
      "Epoch 40/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.1001\n",
      "Epoch 41/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0949\n",
      "Epoch 42/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0904\n",
      "Epoch 43/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.0810\n",
      "Epoch 44/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 1.0769\n",
      "Epoch 45/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0730\n",
      "Epoch 46/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.0695\n",
      "Epoch 47/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 1.0655\n",
      "Epoch 48/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 1.0652\n",
      "Epoch 49/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 1.0602\n",
      "Epoch 50/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0585\n",
      "Epoch 51/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0540\n",
      "Epoch 52/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0524\n",
      "Epoch 53/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0471\n",
      "Epoch 54/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0454\n",
      "Epoch 55/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 1.0405\n",
      "Epoch 56/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0375\n",
      "Epoch 57/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0355\n",
      "Epoch 58/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0311\n",
      "Epoch 59/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0292\n",
      "Epoch 60/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0273\n",
      "Epoch 61/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0242\n",
      "Epoch 62/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0224\n",
      "Epoch 63/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0256\n",
      "Epoch 64/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0190\n",
      "Epoch 65/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0083\n",
      "Epoch 66/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 1.0057\n",
      "Epoch 67/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9966\n",
      "Epoch 68/200\n",
      "531/531 [==============================] - 104s 195ms/step - loss: 0.9898\n",
      "Epoch 69/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9850\n",
      "Epoch 70/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9823\n",
      "Epoch 71/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9775\n",
      "Epoch 72/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9773\n",
      "Epoch 73/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9792\n",
      "Epoch 74/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9732\n",
      "Epoch 75/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9702\n",
      "Epoch 76/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9725\n",
      "Epoch 77/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9643\n",
      "Epoch 78/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9648\n",
      "Epoch 79/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9625\n",
      "Epoch 80/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9605\n",
      "Epoch 81/200\n",
      "531/531 [==============================] - 104s 195ms/step - loss: 0.9594\n",
      "Epoch 82/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9589\n",
      "Epoch 83/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9571\n",
      "Epoch 84/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9536\n",
      "Epoch 85/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9530\n",
      "Epoch 86/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9515\n",
      "Epoch 87/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9511\n",
      "Epoch 88/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9504\n",
      "Epoch 89/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9481\n",
      "Epoch 90/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9454\n",
      "Epoch 91/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9436\n",
      "Epoch 92/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 0.9433\n",
      "Epoch 93/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.9400\n",
      "Epoch 94/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9406\n",
      "Epoch 95/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9370\n",
      "Epoch 96/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 0.9338\n",
      "Epoch 97/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9317\n",
      "Epoch 98/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9308\n",
      "Epoch 99/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9288\n",
      "Epoch 100/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.9287\n",
      "Epoch 101/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9257\n",
      "Epoch 102/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9204\n",
      "Epoch 103/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9204\n",
      "Epoch 104/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9191\n",
      "Epoch 105/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9186\n",
      "Epoch 106/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9177\n",
      "Epoch 107/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.9173\n",
      "Epoch 108/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9125\n",
      "Epoch 109/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9099\n",
      "Epoch 110/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9095\n",
      "Epoch 111/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9068\n",
      "Epoch 112/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.9048\n",
      "Epoch 113/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9037\n",
      "Epoch 114/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9007\n",
      "Epoch 115/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.9012\n",
      "Epoch 116/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8973\n",
      "Epoch 117/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.8953\n",
      "Epoch 118/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 0.8942\n",
      "Epoch 119/200\n",
      "531/531 [==============================] - 102s 192ms/step - loss: 0.9072\n",
      "Epoch 120/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.9146\n",
      "Epoch 121/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8991\n",
      "Epoch 122/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.9029\n",
      "Epoch 123/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8869\n",
      "Epoch 124/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.8846\n",
      "Epoch 125/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.8829\n",
      "Epoch 126/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8820\n",
      "Epoch 127/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8833\n",
      "Epoch 128/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8819\n",
      "Epoch 129/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8790\n",
      "Epoch 130/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8753\n",
      "Epoch 131/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8759\n",
      "Epoch 132/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8753\n",
      "Epoch 133/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8738\n",
      "Epoch 134/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8779\n",
      "Epoch 135/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8804\n",
      "Epoch 136/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8705\n",
      "Epoch 137/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8709\n",
      "Epoch 138/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8726\n",
      "Epoch 139/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8684\n",
      "Epoch 140/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8708\n",
      "Epoch 141/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8725\n",
      "Epoch 142/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8646\n",
      "Epoch 143/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8688\n",
      "Epoch 144/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8658\n",
      "Epoch 145/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8629\n",
      "Epoch 146/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8642\n",
      "Epoch 147/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8618\n",
      "Epoch 148/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8594\n",
      "Epoch 149/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8631\n",
      "Epoch 150/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8576\n",
      "Epoch 151/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8559\n",
      "Epoch 152/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8542\n",
      "Epoch 153/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.8545\n",
      "Epoch 154/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8522\n",
      "Epoch 155/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8550\n",
      "Epoch 156/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8525\n",
      "Epoch 157/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8505\n",
      "Epoch 158/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8489\n",
      "Epoch 159/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8500\n",
      "Epoch 160/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8468\n",
      "Epoch 161/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8469\n",
      "Epoch 162/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8442\n",
      "Epoch 163/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8459\n",
      "Epoch 164/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8487\n",
      "Epoch 165/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8465\n",
      "Epoch 166/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8411\n",
      "Epoch 167/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8529\n",
      "Epoch 168/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.8486\n",
      "Epoch 169/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8399\n",
      "Epoch 170/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.8396\n",
      "Epoch 171/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8396\n",
      "Epoch 172/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8359\n",
      "Epoch 173/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8341\n",
      "Epoch 174/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8345\n",
      "Epoch 175/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.8396\n",
      "Epoch 176/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8361\n",
      "Epoch 177/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8310\n",
      "Epoch 178/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8306\n",
      "Epoch 179/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8299\n",
      "Epoch 180/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8309\n",
      "Epoch 181/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8311\n",
      "Epoch 182/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8299\n",
      "Epoch 183/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8272\n",
      "Epoch 184/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8253\n",
      "Epoch 185/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8271\n",
      "Epoch 186/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8270\n",
      "Epoch 187/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8250\n",
      "Epoch 188/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8249\n",
      "Epoch 189/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8225\n",
      "Epoch 190/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8235\n",
      "Epoch 191/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8243\n",
      "Epoch 192/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8219\n",
      "Epoch 193/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8188\n",
      "Epoch 194/200\n",
      "531/531 [==============================] - 103s 195ms/step - loss: 0.8195\n",
      "Epoch 195/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8256\n",
      "Epoch 196/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8166\n",
      "Epoch 197/200\n",
      "531/531 [==============================] - 103s 194ms/step - loss: 0.8185\n",
      "Epoch 198/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.8162\n",
      "Epoch 199/200\n",
      "531/531 [==============================] - 103s 193ms/step - loss: 0.8151\n",
      "Epoch 200/200\n",
      "531/531 [==============================] - 102s 193ms/step - loss: 0.8177\n"
     ]
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=15,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ");\n",
    "\n",
    "\n",
    "model.fit(dataset, epochs=200, batch_size=BATCH_SIZE,callbacks=[early_stopping])\n",
    "model.save(\"NekIA.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba62fa3",
   "metadata": {},
   "source": [
    "# Make a prediction üé§\n",
    "\n",
    "The first sentence must be in the same format as defined, with only 80 lowercase letters and a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca304687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T23:21:26.453635Z",
     "iopub.status.busy": "2022-10-30T23:21:26.452395Z",
     "iopub.status.idle": "2022-10-30T23:21:31.156599Z",
     "shell.execute_reply": "2022-10-30T23:21:31.155228Z"
    },
    "papermill": {
     "duration": 10.642175,
     "end_time": "2022-10-30T23:21:31.158583",
     "exception": false,
     "start_time": "2022-10-30T23:21:20.516408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START :  avant davoir le respect essaye davoir le mental mais mauvaise mauvaise graine so\n",
      "PREDICTION : us le maitre de ta mere jai les crocs doux soirees en lair est arrete de tourner"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model('NekIA.h5')\n",
    "start = \"avant davoir le respect essaye davoir le mental mais mauvaise mauvaise graine so\"\n",
    "\n",
    "pattern = [char_to_int[c] for c in start]\n",
    "\n",
    "\n",
    "print('START : ',''.join([int_to_char[i] for i in pattern]))\n",
    "print('PREDICTION : ', end='')\n",
    "\n",
    "for i in range(80):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(result, end = '')\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24930.008411,
   "end_time": "2022-10-30T23:22:03.753226",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-30T16:26:33.744815",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
